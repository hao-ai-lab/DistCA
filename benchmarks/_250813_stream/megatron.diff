diff --git a/megatron/core/extensions/transformer_engine.py b/megatron/core/extensions/transformer_engine.py
index fb7711fe..fa689b9f 100644
--- a/megatron/core/extensions/transformer_engine.py
+++ b/megatron/core/extensions/transformer_engine.py
@@ -630,6 +630,7 @@ class TEDotProductAttention(te.pytorch.DotProductAttention):
         v_channels: Optional[int] = None,
         cp_comm_type: str = "p2p",
     ):
+        
         self.config = config
         self.te_forward_mask_type = False
         self.qkv_format: str = 'sbhd'
@@ -762,6 +763,8 @@ class TEDotProductAttention(te.pytorch.DotProductAttention):
         packed_seq_params: PackedSeqParams = None,
     ):
         """Forward."""
+        torch.cuda.nvtx.range_push("TEDotProductAttention.forward")
+        
         packed_seq_kwargs = (
             {key: getattr(packed_seq_params, key) for key in self.kept_packed_seq_params}
             if packed_seq_params is not None
@@ -797,6 +800,7 @@ class TEDotProductAttention(te.pytorch.DotProductAttention):
                 core_attention_bias_type='post_scale_bias', core_attention_bias=attention_bias
             )
 
+        torch.cuda.nvtx.range_push("TEDotProductAttention.forward.core_attn_out")
         if self.te_forward_mask_type:
             if qkv_format == 'thd' and is_te_min_version("1.7.0"):
                 # thd format uses flash attention with cuDNN kernel which requires is_padding=True,
@@ -819,6 +823,9 @@ class TEDotProductAttention(te.pytorch.DotProductAttention):
             core_attn_out = super().forward(
                 query, key, value, attention_mask, **attention_bias_kwargs, **packed_seq_kwargs
             )
+        torch.cuda.nvtx.range_pop()
+        
+        torch.cuda.nvtx.range_pop()
 
         if self.config.apply_rope_fusion and qkv_format == 'bshd':
             return core_attn_out.transpose(0, 1)
diff --git a/megatron/core/transformer/transformer_block.py b/megatron/core/transformer/transformer_block.py
index 032e2cc7..73aefec4 100755
--- a/megatron/core/transformer/transformer_block.py
+++ b/megatron/core/transformer/transformer_block.py
@@ -460,6 +460,7 @@ class TransformerBlock(MegatronModule):
             Union[Tensor, Tuple[Tensor, Tensor]]: The output hidden states tensor of shape
             [s, b, h], and optionally the updated context tensor if cross-attention is used.
         """
+        torch.cuda.nvtx.range_push(f"TransformerBlock.forward")
 
         inference_context = deprecate_inference_params(inference_context, inference_params)
 
@@ -557,6 +558,7 @@ class TransformerBlock(MegatronModule):
                 inp=hidden_states, requires_grad=True, keep_graph=True
             )
 
+        torch.cuda.nvtx.range_pop()
         return hidden_states
 
     def sharded_state_dict(
diff --git a/megatron/core/transformer/transformer_layer.py b/megatron/core/transformer/transformer_layer.py
index 3d62de57..90b6fff1 100644
--- a/megatron/core/transformer/transformer_layer.py
+++ b/megatron/core/transformer/transformer_layer.py
@@ -1,4 +1,5 @@
 # Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.
+import time
 
 import warnings
 from abc import ABC
@@ -365,6 +366,8 @@ class TransformerLayer(MegatronModule, BaseTransformerLayer):
         # self.bias_dropout_add_exec_handler = nullcontext if use_nvfuser else torch.enable_grad
         self.bias_dropout_add_exec_handler = torch.enable_grad
 
+        self.rank = None
+
     @staticmethod
     def _get_layer_offset(config: TransformerConfig):
         """
@@ -386,8 +389,37 @@ class TransformerLayer(MegatronModule, BaseTransformerLayer):
         This method calls the core computation of a transformer layer, including
         self-attention, cross-attention (if applicable), and feed-forward operations.
         """
+        if self.rank is None:
+            self.rank = torch.distributed.get_rank()
+
+        layer_number = self.layer_number
+        torch.cuda.nvtx.range_push(f"TransformerLayer.forward[{layer_number}]")
+
+        torch.cuda.nvtx.range_push(f"TransformerLayer.forward.attention[{layer_number}]")
+        torch.cuda.synchronize()
+        start_time = time.time()
         pre_mlp_layernorm_output, residual, context = self._forward_attention(*args, **kwargs)
+        torch.cuda.synchronize()
+        end_time = time.time()
+        duration = end_time - start_time
+        duration_ms = duration * 1000
+        if self.rank % 8 == 0:
+            print(f"[Rank {self.rank}] TransformerLayer.forward.attention[{layer_number}] duration: {duration_ms:.3f} ms")
+        torch.cuda.nvtx.range_pop()
+
+        torch.cuda.nvtx.range_push(f"TransformerLayer.forward.mlp[{layer_number}]")
+        torch.cuda.synchronize()
+        start_time = time.time()
         output = self._forward_mlp(pre_mlp_layernorm_output, residual)
+        torch.cuda.synchronize()
+        end_time = time.time()
+        duration = end_time - start_time
+        duration_ms = duration * 1000
+        if self.rank % 8 == 0:
+            print(f"[Rank {self.rank}] TransformerLayer.forward.mlp[{layer_number}] duration: {duration_ms:.3f} ms")
+        torch.cuda.nvtx.range_pop()
+
+        torch.cuda.nvtx.range_pop()
         return output, context
 
     def _forward_attention(
@@ -437,6 +469,7 @@ class TransformerLayer(MegatronModule, BaseTransformerLayer):
         residual = hidden_states
 
         # Optional Input Layer norm
+        torch.cuda.nvtx.range_push("TransformerLayer._forward_attention.input_layernorm")
         if self.recompute_input_layernorm:
             self.input_layernorm_checkpoint = tensor_parallel.CheckpointWithoutOutput()
             input_layernorm_output = self.input_layernorm_checkpoint.checkpoint(
@@ -444,8 +477,10 @@ class TransformerLayer(MegatronModule, BaseTransformerLayer):
             )
         else:
             input_layernorm_output = self.input_layernorm(hidden_states)
+        torch.cuda.nvtx.range_pop()
 
         # Self attention.
+        torch.cuda.nvtx.range_push("TransformerLayer._forward_attention.self_attention")
         attention_output_with_bias = self.self_attention(
             input_layernorm_output,
             attention_mask=attention_mask,
@@ -457,6 +492,7 @@ class TransformerLayer(MegatronModule, BaseTransformerLayer):
             packed_seq_params=packed_seq_params,
             sequence_len_offset=sequence_len_offset,
         )
+        torch.cuda.nvtx.range_pop()
 
         if self.recompute_input_layernorm:
             # discard the output of the input layernorm and register the recompute
@@ -467,17 +503,22 @@ class TransformerLayer(MegatronModule, BaseTransformerLayer):
 
         # TODO: could we move `bias_dropout_add_exec_handler` itself
         # inside the module provided in the `bias_dropout_add_spec` module?
+        torch.cuda.nvtx.range_push("TransformerLayer._forward_attention.self_attn_bda")
         with self.bias_dropout_add_exec_handler():
             hidden_states = self.self_attn_bda(self.training, self.config.bias_dropout_fusion)(
                 attention_output_with_bias, residual, self.hidden_dropout
             )
+        torch.cuda.nvtx.range_pop()
 
         # Residual connection.
         residual = hidden_states
 
         # Optional Layer norm after self-attention
+        torch.cuda.nvtx.range_push("TransformerLayer._forward_attention.pre_cross_attn_layernorm")
         pre_cross_attn_layernorm_output = self.pre_cross_attn_layernorm(hidden_states)
+        torch.cuda.nvtx.range_pop()
 
+        torch.cuda.nvtx.range_push("TransformerLayer._forward_attention.cross_attention")
         # Cross attention.
         attention_output_with_bias = self.cross_attention(
             pre_cross_attn_layernorm_output,
@@ -485,17 +526,21 @@ class TransformerLayer(MegatronModule, BaseTransformerLayer):
             key_value_states=context,
             inference_context=inference_context,
         )
+        torch.cuda.nvtx.range_pop()
 
         if isinstance(attention_output_with_bias, dict) and "context" in attention_output_with_bias:
             context = attention_output_with_bias["context"]
 
         # TODO: could we move `bias_dropout_add_exec_handler` itself
         # inside the module provided in the `bias_dropout_add_spec` module?
+        torch.cuda.nvtx.range_push("TransformerLayer._forward_attention.cross_attn_bda")
         with self.bias_dropout_add_exec_handler():
             hidden_states = self.cross_attn_bda(self.training, self.config.bias_dropout_fusion)(
                 attention_output_with_bias, residual, self.hidden_dropout
             )
+        torch.cuda.nvtx.range_pop()
 
+        torch.cuda.nvtx.range_push("TransformerLayer._forward_attention.residual")
         # Residual connection.
         residual = hidden_states
 
@@ -507,6 +552,7 @@ class TransformerLayer(MegatronModule, BaseTransformerLayer):
             )
         else:
             pre_mlp_layernorm_output = self.pre_mlp_layernorm(hidden_states)
+        torch.cuda.nvtx.range_pop()
 
         return pre_mlp_layernorm_output, residual, context
 
@@ -523,27 +569,34 @@ class TransformerLayer(MegatronModule, BaseTransformerLayer):
         """
 
         # MLP.
+        torch.cuda.nvtx.range_push("TransformerLayer._forward_mlp.mlp")
         if self.recompute_mlp:
             mlp_output_with_bias = tensor_parallel.checkpoint(
                 self.mlp, False, pre_mlp_layernorm_output
             )
         else:
             mlp_output_with_bias = self.mlp(pre_mlp_layernorm_output)
+        torch.cuda.nvtx.range_pop()
 
+        torch.cuda.nvtx.range_push("TransformerLayer._forward_mlp.recompute_pre_mlp_layernorm")
         if self.recompute_pre_mlp_layernorm:
             # discard the output of the pre-mlp layernorm and register the recompute
             # as a gradient hook of mlp_output_with_bias[0]
             self.pre_mlp_norm_checkpoint.discard_output_and_register_recompute(
                 mlp_output_with_bias[0]
             )
+        torch.cuda.nvtx.range_pop()
 
+        torch.cuda.nvtx.range_push("TransformerLayer._forward_mlp.mlp_bda")
         # TODO: could we move `bias_dropout_add_exec_handler` itself
         # inside the module provided in the `bias_dropout_add_spec` module?
         with self.bias_dropout_add_exec_handler():
             hidden_states = self.mlp_bda(self.training, self.config.bias_dropout_fusion)(
                 mlp_output_with_bias, residual, self.hidden_dropout
             )
+        torch.cuda.nvtx.range_pop()
 
+        torch.cuda.nvtx.range_push("TransformerLayer._forward_mlp.make_viewless_tensor")
         # Jit compiled function creates 'view' tensor. This tensor
         # potentially gets saved in the MPU checkpoint function context,
         # which rejects view tensors. While making a viewless tensor here
@@ -553,7 +606,7 @@ class TransformerLayer(MegatronModule, BaseTransformerLayer):
         output = make_viewless_tensor(
             inp=hidden_states, requires_grad=hidden_states.requires_grad, keep_graph=True
         )
-
+        torch.cuda.nvtx.range_pop()
         return output
 
     def sharded_state_dict(
@@ -586,6 +639,7 @@ class TransformerLayer(MegatronModule, BaseTransformerLayer):
         Returns:
             Dict[str, torch.Tensor]: A dictionary containing the static inputs for the layer.
         """
+        torch.cuda.nvtx.range_push("TransformerLayer.get_layer_static_inputs")
         # Calculate data shape related values.
         context_parallel_size = self.config.context_parallel_size
         slen_per_cp = seq_length // context_parallel_size
@@ -608,6 +662,7 @@ class TransformerLayer(MegatronModule, BaseTransformerLayer):
             .reshape(1, 1, slen_per_cp, seq_length)
             .tile(micro_batch_size, 1, 1, 1)
         )
+        torch.cuda.nvtx.range_pop()
         return static_inputs
 
     def setup_manual_hooks(self, make_hook_func):