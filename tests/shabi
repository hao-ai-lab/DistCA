unwrap_model(self.train_module[0]).decoder.layers[0].self_attention.linear_qkv.weight.main_grad=tensor([[-1.4087e+19, -5.3205e+18,  4.4744e+18,  ..., -2.2874e+18,
          9.0559e+18,  1.5478e+19],
        [ 1.8529e+19,  4.1852e+18, -6.7253e+18,  ...,  1.1586e+19,
         -4.9060e+18,  6.9110e+18],
        [ 2.1133e+17, -2.9112e+19,  4.8735e+18,  ..., -3.6298e+18,
          5.3255e+18,  9.6141e+18],
        ...,
        [-1.9503e+19,  2.5304e+19, -1.2500e+19,  ...,  4.5912e+19,
         -2.7521e+19, -2.5472e+18],
        [ 8.9807e+18,  9.4842e+18,  2.4085e+18,  ...,  7.4415e+18,
         -1.0888e+19, -3.5252e+19],
        [ 8.3586e+17,  1.8730e+19, -4.6892e+18,  ...,  4.0305e+19,
         -3.1850e+19, -2.5465e+18]], device='cuda:0')


unwrap_model(self.train_module[0]).decoder.layers[0].self_attention.linear_qkv.weight.main_grad=tensor([[ 0.1787, -0.7148,  0.9102,  ...,  1.3750,  0.4551, -0.5078],
        [-0.0569,  0.9375, -0.9258,  ..., -1.1250, -0.5312,  0.5117],
        [-0.1787,  0.7812, -0.8867,  ..., -1.1797, -0.3945,  0.5859],
        ...,
        [ 0.1758,  0.3320, -0.1680,  ..., -0.1357, -0.0649, -0.8516],
        [ 0.5781,  0.5938,  0.2178,  ..., -0.5273,  0.8750, -0.4707],
        [ 0.7656,  1.3203,  0.0095,  ..., -1.0078,  1.5859, -0.9297]],
       device='cuda:1')
====================forward_backward_batch attention server, done
losses_reduced=[]
unwrap_model(self.train_module[0]).decoder.layers[0].self_attention.linear_qkv.weight.main_grad=tensor([[ -3.8750,  -2.2031,  -2.5156,  ...,   1.3750,  -3.3125,   0.6641],
        [  4.0312,  -0.2041,  -0.2314,  ...,  -3.6875,   3.6875,  -2.3906],
        [  2.1562,   2.4375,  -2.2656,  ...,   4.8750,  -2.4375,   3.8594],
        ...,
        [  8.5625,   3.6406,  -2.0938,  ...,  -1.4688,  -1.0781,   7.6875],
        [ -9.1875,  -0.9258, -11.0625,  ...,   9.8125,  -2.8906, -15.5000],
        [  1.5312,  -2.5469,   2.6719,  ...,   0.5977,   6.4375, -16.2500]],
       device='cuda:0')

unwrap_model(self.train_module[0]).decoder.layers[0].self_attention.linear_proj.weight.main_grad=tensor([[ 0.0058, -0.0040,  0.0118,  ..., -0.0057, -0.0040, -0.0040],
        [ 0.0023,  0.0162, -0.0053,  ...,  0.0117, -0.0014, -0.0006],
        [ 0.0142, -0.0027, -0.0058,  ..., -0.0091, -0.0082, -0.0013],
        ...,
        [ 0.0039, -0.0051, -0.0052,  ..., -0.0028,  0.0028, -0.0061],
        [-0.0048, -0.0077,  0.0024,  ...,  0.0040,  0.0064, -0.0143],
        [ 0.0034,  0.0219, -0.0060,  ...,  0.0005, -0.0075,  0.0247]],
       device='cuda:1')
====================forward_backward_batch attention server, done
losses_reduced=[]
unwrap_model(self.train_module[0]).decoder.layers[0].self_attention.linear_proj.weight.main_grad=tensor([[-0.0635, -0.0286, -0.0229,  ...,  0.0211,  0.0625, -0.0305],
        [-0.0232, -0.0732,  0.0403,  ...,  0.0264, -0.0571, -0.0198],
        [-0.0374, -0.0315,  0.0391,  ..., -0.0276, -0.0209,  0.0330],
        ...,
        [ 0.0547,  0.0396, -0.0032,  ...,  0.0791, -0.0240, -0.0135],
        [ 0.0113,  0.0022,  0.0383,  ...,  0.0330,  0.0469, -0.0815],
        [-0.0153, -0.0306, -0.0786,  ...,  0.0064,  0.0474, -0.0072]],
       device='cuda:0')